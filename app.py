import gradio as gr
import os
import tempfile
from pathlib import Path

# Fix python path just in case
import sys
sys.path.insert(0, str(Path(__file__).parent))

from config import SUPPORTED_NOVEL_TYPES, MAX_REVISIONS
from main import run_single_chunk, merge_chunk_results
from utils.chunker import split_into_chunks, get_chunk_info

import config

def generate_script(text_input, file_input, novel_type, llm_provider, llm_model, api_key, chunk_size, chunk_overlap):
    if not text_input and not file_input:
        yield "è¯·æä¾›æ–‡æœ¬æˆ–ä¸Šä¼ txtæ–‡ä»¶"
        return
    
    novel_text = ""
    if text_input:
        novel_text += text_input + "\n"
        
    if file_input:
        try:
            # Gradio file objects usually have a 'name' attribute or it's a temp path
            file_path = file_input if isinstance(file_input, str) else file_input.name
            with open(file_path, 'r', encoding='utf-8') as f:
                novel_text += f.read()
        except Exception as e:
            yield f"è¯»å–æ–‡ä»¶å¤±è´¥: {e}"
            return

    novel_text = novel_text.strip()
    if not novel_text:
        yield "æ–‡æœ¬å†…å®¹ä¸ºç©ºï¼"
        return

    # Setup environment variables dynamically
    config.LLM_PROVIDER = llm_provider
    os.environ["LLM_PROVIDER"] = llm_provider

    if llm_provider == "openai":
        if api_key:
            os.environ["OPENAI_API_KEY"] = api_key
            config.OPENAI_API_KEY = api_key
    elif llm_provider == "gemini":
        if api_key:
            os.environ["GOOGLE_API_KEY"] = api_key
            config.GOOGLE_API_KEY = api_key            
    
    if llm_model:
        os.environ["LLM_MODEL"] = llm_model
        config.LLM_MODEL = llm_model
        
    # Optional: we can disable HUMAN_REVIEW to ensure it doesn't block
    config.HUMAN_REVIEW = False

    info = get_chunk_info(novel_text, chunk_size)
    chunks = split_into_chunks(novel_text, chunk_size, chunk_overlap)
    
    chunk_states = []
    scene_offset = 0
    yield f"ğŸš€ å¼€å§‹å¤„ç†ï¼Œå…±åˆ†ä¸º {len(chunks)} æ®µ (æ€»å­—æ•°: {info['total_chars']})...\n"
    
    output_log = f"ğŸš€ å¼€å§‹å¤„ç†ï¼Œå…±åˆ†ä¸º {len(chunks)} æ®µ (æ€»å­—æ•°: {info['total_chars']})...\n\n"
    
    try:
        for i, chunk in enumerate(chunks):
            log_msg = f"â³ æ­£åœ¨å¤„ç†ç¬¬ {i + 1}/{len(chunks)} æ®µ (å­—æ•°: {len(chunk)}, ä»åç§»é‡ {scene_offset + 1} å¼€å§‹)...\n"
            output_log += log_msg
            yield output_log
            
            # è¿è¡Œå·¥ä½œæµ
            state = run_single_chunk(chunk, novel_type, scene_offset)
            chunk_states.append(state)
            
            scene_count = len(state.get("screenplay_scenes", []))
            scene_offset += scene_count
            
            log_msg = f"âœ… ç¬¬ {i + 1} æ®µå¤„ç†å®Œæˆï¼Œæœ¬æ®µåœºæ™¯æ•°: {scene_count}ï¼Œç´¯ç§¯åœºæ™¯æ•°: {scene_offset}\n\n"
            output_log += log_msg
            yield output_log
            
    except Exception as e:
        import traceback
        err = traceback.format_exc()
        yield output_log + f"\nâŒ å¤„ç†å‡ºé”™:\n{err}"
        return

    if len(chunk_states) == 1:
        final_state = chunk_states[0]
    else:
        log_msg = f"ğŸ”— æ­£åœ¨åˆå¹¶ {len(chunks)} æ®µç»“æœ...\n"
        output_log += log_msg
        yield output_log
        final_state = merge_chunk_results(chunk_states, novel_type)
        
    log_msg = f"\nğŸ‰ ç”Ÿæˆå®Œæˆï¼\n"
    output_log += log_msg
    yield output_log
    
    final_script = final_state.get("final_script", "ï¼ˆæ— æœ€ç»ˆå‰§æœ¬è¾“å‡ºï¼‰")
    yield final_script


# Custom Theme Config
custom_theme = gr.themes.Soft(
    primary_hue="indigo",
    secondary_hue="purple",
    neutral_hue="slate",
    spacing_size="lg",
    radius_size="lg",
    font=[gr.themes.GoogleFont("Inter"), "system-ui", "sans-serif"]
).set(
    body_background_fill="*neutral_50",
    body_text_color="*neutral_900",
    button_primary_background_fill="*primary_600",
    button_primary_background_fill_hover="*primary_700",
    border_color_primary="*primary_200",
)

with gr.Blocks(title="æ¼«å‰§æ™ºèƒ½å‰§æœ¬ç”Ÿæˆ") as demo:
    
    gr.HTML('''
        <div style="padding: 10px 0;">
            <div class="title-text">æ¼«å‰§æ™ºèƒ½å‰§æœ¬ç”Ÿæˆå¼•æ“</div>
            <div class="subtitle-text">å°†å°è¯´åŸè‘—æ–‡æœ¬è½¬åŒ–ä¸ºç»“æ„åŒ–çš„æ¼«å‰§åˆ†é•œå‰§æœ¬æµç¨‹å¼•æ“</div>
        </div>
    ''')
    
    with gr.Row():
        with gr.Column(scale=9, elem_classes="block-container"):
            gr.Markdown("### ğŸ“ ç¬¬ä¸€æ­¥ï¼šè¾“å…¥æ–‡ç¨¿", elem_classes="markdown-header")
            with gr.Tabs():
                with gr.TabItem("ğŸ“‹ ç›´æ¥ç²˜è´´"):
                    text_input = gr.Textbox(
                        lines=10, 
                        label="å°è¯´åŸæ–‡ï¼ˆå¼•æ“å°†ä¼šè¯»å–è¿›è¡Œè§£æï¼‰", 
                        placeholder="åœ¨æ­¤ç²˜è´´å°è¯´å†…å®¹...\næ”¯æŒè¶…é•¿æ–‡æœ¬ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨åœ¨åå°åˆ†æ®µå¹¶åˆå¹¶å¤„ç†ã€‚"
                    )
                with gr.TabItem("ğŸ“‚ é™„ä»¶ä¸Šä¼ "):
                    file_input = gr.File(
                        label="ä¸Šä¼  txt æ–‡ä»¶", 
                        file_types=[".txt"],
                        height=200
                    )
                    
            with gr.Row():
                submit_btn = gr.Button("ğŸš€ å¯åŠ¨å¼•æ“å¼€å§‹ç”Ÿæˆ", variant="primary", size="lg")
                
        with gr.Column(scale=4, elem_classes="block-container"):
            gr.Markdown("### âš™ï¸ ç¬¬äºŒæ­¥ï¼šå‚æ•°é…ç½®", elem_classes="markdown-header")
            
            with gr.Group():
                novel_type = gr.Dropdown(choices=SUPPORTED_NOVEL_TYPES, value="ä»™ä¾ /ç„å¹»", label="ğŸ­ å°è¯´é¢˜æ")
                llm_provider = gr.Dropdown(choices=["openai", "gemini"], value="openai", label="ğŸ¤– å¤§è¯­è¨€æ¨¡å‹æº")
                llm_model = gr.Textbox(value="gpt-4o", label="ğŸ§® æ¨¡å‹åç§° (e.g. gpt-4o, gemini-2.5-flash)")
                api_key = gr.Textbox(value="", label="ğŸ”‘ API Key (å¦‚å·²é…ç¯å¢ƒå˜é‡åˆ™ç•™ç©º)", type="password")
            
            with gr.Accordion("ğŸ› ï¸ è¿›é˜¶åˆ†ç‰‡æ§åˆ¶", open=False):
                gr.Markdown("ä¸ºé˜²æ­¢è¶…å‡º Tokens é™åˆ¶ï¼Œé•¿æ–‡ä¼šè‡ªåŠ¨å¼€å¯åˆ‡ç‰‡å·¥ä½œæµï¼š")
                chunk_size = gr.Slider(minimum=500, maximum=10000, value=2000, step=100, label="å•ä½åˆ‡ç‰‡å­—ç¬¦æ•° (Chunk Size)")
                chunk_overlap = gr.Slider(minimum=0, maximum=1000, value=200, step=50, label="ä¿ç•™ä¸Šä¸‹æ–‡å­—ç¬¦æ•° (Chunk Overlap)")
                
    with gr.Row():
        with gr.Column(scale=1, elem_classes="block-container"):
            gr.Markdown("### ğŸ¯ ç”Ÿæˆé¢æ¿", elem_classes="markdown-header")
            output_text = gr.Textbox(
                lines=25, 
                label="å®æ—¶å·¥ä½œæµæ—¥å¿— & æœ€ç»ˆå‰§æœ¬äº§ç‰©"
            )

    submit_btn.click(
        fn=generate_script,
        inputs=[text_input, file_input, novel_type, llm_provider, llm_model, api_key, chunk_size, chunk_overlap],
        outputs=output_text
    )

if __name__ == "__main__":
    demo.queue().launch(
        server_name="0.0.0.0", 
        show_error=True,
        theme=custom_theme,
        css="""
        .title-text { text-align: center; font-size: 2.2em; font-weight: 700; color: #1e1e1e; margin-bottom: 0px !important; }
        .subtitle-text { text-align: center; color: #555555; font-size: 1.1em; margin-top: 5px !important; margin-bottom: 25px !important; }
        .block-container { box-shadow: 0 4px 6px -1px rgb(0 0 0 / 0.1), 0 2px 4px -2px rgb(0 0 0 / 0.1); border-radius: 10px; background-color: white; padding: 15px; margin-bottom: 20px; }
        .markdown-header { margin-bottom: 15px !important; }
        """
    )


